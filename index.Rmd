---
title: "Practical Machine Learning Course Project"
author: "Tanmoy Rath"
date: "16 January 2019"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Dataset Background

The weight-lifting dataset used in this analysis was provided by <a href="http://groupware.les.inf.puc-rio.br/">Groupware@LES</a>. You can find more information about it on:

+ <http://groupware.les.inf.puc-rio.br/har> (see the section on the Weight Lifting Exercise Dataset)
+ <http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201> (their research paper using the dataset)

6 young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions:

+ **Class A**, exactly according to the specification,
+ **Class B**, throwing the elbows to the front,
+ **Class C**, lifting the dumbbell only halfway,
+ **Class D**, lowering the dumbbell only halfway &
+ **Class E**, throwing the hips to the front.

Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes.

**The goal of my project is to predict the manner in which they did the exercise** or in other words, to determine the class. This is to be determined by analyzing the data from various accelerometers on the belt, forearm, arm, and dumbell of the 6 participants.





## Data Cleaning

```{r, echo=TRUE}
training <- read.csv(file = "pml-training.csv", na.strings = c("","NA"))
```

On looking at the dataset, I found that a **lot of columns containd NA values**, before doing any imputation, I calculated the % of NA per column. I stored the result in NA_Colmns variable.

```{r, echo=TRUE}
NA_Colmns <- 100 * colSums(is.na(training)) / (dim(training)[1])
```

**Those columns that have more than 80% values as NA, were removed them from the dataset**.

```{r, echo=TRUE}
NA_Colmns <- NA_Colmns > 80
training <- training[, ! NA_Colmns]
```

**Next, I removed the columns that measured time (timestamps & window), sequences(X) and User Names, because such data would confound the prediction which was to be done solely on data from accelerometers.**

```{r, echo=TRUE}
drops <- c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2",
           "cvtd_timestamp", "new_window", "num_window")

remv_colms <- colnames(training) %in% drops
training <- training[, !remv_colms]
```

I also checked for any **near zero variance predictors** and found none.

```{r, echo=TRUE}
suppressMessages(library(caret))
nearZeroVar(training,saveMetrics=FALSE)
```





## Data Exploration

**Density plots of columns**<br>
Some of the columns of training data are highly skewed, indicating presence of outliers. They are (with outlier values):

+ gyros_dumbbell_x (-204), gyros_dumbbell_y (52), gyros_dumbbell_z (317)
+ gyros_forearm_x (-22), gyros_forearm_y (311), gyros_forearm_z (231)
+ magnet_dumbbell_y (-3600)

2 of the density plots are shown below (with outlier values).

<table><tr><td>
```{r, echo=FALSE}
#par(mfrow = c(1,2))

dens <- density(training$gyros_dumbbell_x)
plot(dens, main="gyros_dumbbell_x (-204)")
polygon(dens, col="#7F7FFF", border="black")
```
</td><td>
```{r, echo=FALSE}
dens <- density(training$magnet_dumbbell_y)
plot(dens, main="magnet_dumbbell_y (-3600)")
polygon(dens, col="#7F7FFF", border="black")
```
</td></tr></table>

#### Outliers Treatment

Only 2 rows were found containing these outliers. Therefore, those rows containg these outliers were removed. The loss in data is negligible considering there are 19620 remaining observations.<br>
```{r, echo=TRUE}
training <- training[training$gyros_dumbbell_x != -204,]
training <- training[training$gyros_dumbbell_y != 52,]
training <- training[training$gyros_dumbbell_z != 317,]

training <- training[training$gyros_forearm_x != -22,]
training <- training[training$gyros_forearm_y != 311,]
training <- training[training$gyros_forearm_z != 231,]

training <- training[training$magnet_dumbbell_y != -3600,]
```

**After cleaning, the new training dataset contained only 53 columns out of 19620 observations.**

```{r, echo=TRUE}
str(training)
```





## Data Splitting

Since the **pml-testing.csv** file doesnot contain any classe column, it could not be used for testing purposes. Hence the training data was split into **myTrain** and **myTest** of 0.75 and 0.25 parts respectively.

+ **training** = **75%**(=myTrain) + **25%**(=myTest)

```{r, echo=TRUE}
set.seed(33492)
library(caret)
inTrain <- createDataPartition( y=training$classe, p=0.75, list = FALSE )
myTrain <- training[inTrain,]
myTest <- training[-inTrain,]
```





## Model : Random Forest

+ The myTrain data was cross-validated 7 times on the optimal random forest, and from that, the out of sample error was calculated. I used cross-validation of K = 7 so that it isnt very high to introduce more variance, neither too low to introduce any bias.
+ ntree = 400 was used because it had lesser prediction error on myTest, than default ntree=500.
+ Setting importance to TRUE allowed the model to build efficient tress based on variable importance which reduces error.
+ I also used parallel processing to reduce the time needed to build the model.

#### Parallel Processing

Using the parallel and doParallel packages, it is possible to build trees in parallel.

```{r, echo=TRUE}
set.seed(33492)
suppressMessages(library(parallel))
suppressMessages(library(doParallel))
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)

fitControl <- trainControl(method = "cv", number = 7, returnData = FALSE, allowParallel = TRUE)

modelFit <- train(classe ~.,
                  data = myTrain,
                  method = "rf",
                  ntree = 400,
                  importance = TRUE,
                  trControl = fitControl)

stopCluster(cluster)
registerDoSEQ()
```


## Model's Accuracy

The accuracy of the model has been measured in various ways.

#### 1. Optimal "mtry" accuracy = 99.30695%<br>
```{r, echo=TRUE}
modelFit
```


#### 2. Average cross-validation accuracy = 99.30695%<br>
The accuracy obtained by averaging the accuracy of each cross-validation fold, which is same as Optimal "mtry" Accuracy.<br>
```{r, echo=TRUE}
cv_folds <- modelFit$resample
cv_folds
acc <- mean(  cv_folds$Accuracy  )*100
acc
```


#### 3. Out of sample error = 0.6930462%<br>
This can be calculated as 100 - average cross validation accuracy i.e. **100 - acc**.<br>
```{r, echo=TRUE}
100 - acc
```


#### 4. Final Model OOB error rate = 0.58%<br>
This is very very close to the out of sample error.<br>
```{r, echo=TRUE}
modelFit$finalModel
```


#### 5. Accuracy on the whole training set = 99.89%<br>
```{r, echo=TRUE}
confusionMatrix(   training$classe,   predict( modelFit$finalModel, newdata = training )   )
```


#### Accuracy Summary

+ **Optimal "mtry" accuracy = 99.30695%**
+ **Average cross-validation accuracy = 99.30695%**
+ **Out of sample error = 0.6930462%** (100 - average cross validation accuracy)
+ **Final Model OOB error rate = 0.58%**
+ **Accuracy on the whole training set = 99.89%**


## Model Plots

```{r, echo=TRUE}
plot( modelFit$finalModel, main="Error vs No. of Trees" )
```

```{r, echo=FALSE}
suppressMessages(library(randomForest))
#varImpPlot(modelFit$finalModel, main="Variable Importance Plot")
#plot(modelFit, metric = "Kappa", main="Kappa vs no. of Predictors")
```

```{r, echo=TRUE}
plot(modelFit, main="Accuracy vs no. of Predictors")
```





## Prediction

#### 1. Prediction accuracy on myTest

Prediction accuracy on myTest was found to be **99.55%**.
```{r, echo=TRUE}
confusionMatrix(   myTest$classe,   predict( modelFit$finalModel, newdata = myTest )   )
```

#### 2. Prediction on pml-testing.csv

The pml data was first cleaned. After that the final model was applied for prediction.<br>
```{r, echo=TRUE}
testing <- read.csv(file = "pml-testing.csv", na.strings = c("","NA"))

NA_Colmns <- (     colSums(is.na(testing)) / (dim(testing)[1])     ) > 0.8
testing <- testing[, !NA_Colmns]

drops <- c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2",
           "cvtd_timestamp", "new_window", "num_window", "problem_id")
remv_colms <- colnames(testing) %in% drops
testing <- testing[, !remv_colms]



predict( modelFit$finalModel ,newdata = testing )
```





## Conclusion

The random forest is a very powerful machine learning algorithm. If time permits, with properly tuned parameters, the model's accuracy can be greately amplified. On the training set (pml-training.csv), my model's accuracy was found to be **99.89%**, while the cross-validated(k=7) out of sample error was found to be **0.6930462%**.